# -*- coding: utf-8 -*-
"""05_5_1_Classificacao_Árvore_de_Decisão_Aula.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11j5kqbi8mATjYxK_7tIk0LpIvqAUjA77

# Disciplina de Mineração de Dados

Universidade Federal de Sergipe, Campus Prof. Alberto Carvalho - Itabaiana

Professores:

- Raphael Silva Fontes

- Prof. Dr. Methanias Colaço Rodrigues Júnior

## Árvore de Decisão

![](https://miro.medium.com/max/700/0*Wy3QjtXL9qf-Ssyz.jpg)

Um dos mais conhecidos e mais utilizados modelos de aprendizado de máquina utilizados no mercado. 
É um algoritmo supervisionado que se baseia na divisão dos dados em grupos homogêneos para a melhor classificação (classe) ou regressão (variável contínua). Consegue tomar decisão, a partir da extração de caracteristicas dos dados.


### De forma geral

- Classificação e Regressão;
- Fácil Entendimento;
- Útil para Exploração dos Dados;
- Reequer menos tarefa de limpeza de dados;
- Aplicável a diferentes tipos de dados;
- Propenso a sofrer _overfitting_; #Sobre carga
- Instáveis, pequenas alterações produzem novas árvores.

---

![](https://web.tecnico.ulisboa.pt/ana.freitas/bioinformatics.ath.cx/bioinformatics.ath.cx/uploads/RTEmagicC_arv_dec2.gif.gif)

---

![](https://i.imgur.com/FdiGksQ.png)


### Terminologias

- Nó raiz;
- Nós de decisão;
- Branch/ramo/sub-árvore;
- Folha/Nó terminal.

### Como controlar o overfitting?

- Número mínimo de amostras para divisão do nó:
    - Valores altos previnem a criação de modelos complexos ou podem causar underfitting
- Número mínimo de amostras para o nível folha:
    - Controla o crescimento da árvore e deve ser considerado em cenários com dados desbalanceados.
- Profundidade máxima da árvore:
- Número máximo de _features_ (caracteristicas, colunas) para considerar durante a divisão.

![](https://minerandodados.com.br/wp-content/uploads/2020/01/image-102.png)

## Aplicando
"""

import pandas as pd
import matplotlib.pyplot as plt

from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split

df = pd.read_csv('/content/titanic.csv')

df.head()

df.info()

df['Sex'].unique()

df = df.loc[df['Age'].notna()]

df['Sex'] = df['Sex'].apply(lambda row: 1 if row == 'male' else 0)

colunas_excluidas = ['PassengerId', 'Survived', 'Pclass', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin', 'Embarked']
colunas_utilizadas = ['Age', 'Sex', 'Fare']

X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=colunas_excluidas), df['Survived'], test_size=0.3)

X_train.shape, X_test.shape

y_train.shape, y_test.shape

clf = DecisionTreeClassifier(max_depth=2)

clf = clf.fit(X_train, y_train)

clf.feature_importances_

for feature, importancia in zip(colunas_utilizadas, clf.feature_importances_):
    print(f'{feature}: {importancia}')

resultado = clf.predict(X_test)

resultado

print(metrics.classification_report(y_test, resultado))

plt.figure(figsize=(25, 10))
titanic_tree = plot_tree(clf, feature_names=colunas_utilizadas, class_names=['Não Sobreviveu', 'Sobreviveu'], 
                         filled=True, rounded=True, fontsize=14)

